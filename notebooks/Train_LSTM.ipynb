{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from textblob import Word\n",
    "from collections import Counter\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train_data.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_lstm = pd.read_csv('../LSTM/LSTM_details/stopwords_lstm.csv')\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    stopwords = set(STOPWORDS)\n",
    "    \n",
    "    # Appending new airline related stop-words\n",
    "    stopwords.update([str(i).lower() for i in stopwords_lstm.name]) \n",
    "    \n",
    "    # Filter for mentions\n",
    "    mentions_filter = re.compile(r'(?<=\\@)(\\w+)')\n",
    "    \n",
    "    # Filter for hash-tags\n",
    "    hashtags_filter = re.compile(r'(?<=\\#)(\\w+)')\n",
    "    \n",
    "    # Filter for flights numbers\n",
    "    flight_numbers = re.compile(r'(flt\\d*)')\n",
    "    \n",
    "    # Finding all mentions\n",
    "    all_mentions = mentions_filter.findall(text.lower())\n",
    "    \n",
    "    # Finding all hash-tags\n",
    "    all_hashtag = hashtags_filter.findall(text.lower())\n",
    "    \n",
    "    # Finding all hash-tags\n",
    "    all_flights = flight_numbers.findall(text.lower())\n",
    "    \n",
    "    word_lemmatize = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatizing \n",
    "    def lemm_tokens(tokens, lemmatize):\n",
    "        lemmatized = []\n",
    "        for item in tokens:\n",
    "            lemmatized.append(lemmatize.lemmatize(item,'v'))\n",
    "        return lemmatized\n",
    "    \n",
    "    # De-emojify tweets to text\n",
    "    def deEmojify(inputString):\n",
    "        return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    text = deEmojify(text)\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Removing punctuation\n",
    "    punc_text = [x.lower() for x in text if x not in string.punctuation]\n",
    "    text = \"\".join(punc_text)\n",
    "    \n",
    "    # Tokenize words\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Keeping the words with length between 2 and 15\n",
    "    filtered_tokens = [x for x in word_tokens if len(x)>2 and len(x)<15]\n",
    "    \n",
    "    # Filter tokens\n",
    "    tokens = lemm_tokens(filtered_tokens, word_lemmatize)\n",
    "    all_tokens = [i for i in tokens if (i not in stopwords) and (i not in all_mentions) \n",
    "                  and (i not in all_hashtag) and (i not in all_flights) and (not i.isdigit())]\n",
    "    \n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_tokenized']=train_df['text'].astype(\"string\").map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_if_empty(df):\n",
    "    empty_ids = df['text_tokenized'].map(lambda i: len(i)) > 0\n",
    "    idsToDelete = []\n",
    "    for i in range(len(empty_ids)):\n",
    "        if not empty_ids[i]:\n",
    "            idsToDelete.append(i)\n",
    "    for currentID in idsToDelete:\n",
    "        df = df.drop(df[df.index == currentID].index[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df = remove_if_empty(train_df)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelEncoder()\n",
    "train_df['airline_sentiment'] = lb.fit_transform(train_df['airline_sentiment'])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000, split=' ') \n",
    "tokenizer.fit_on_texts(train_df['text_tokenized'].values)\n",
    "X = tokenizer.texts_to_sequences(train_df['text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 128, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#Splitting the data into training and testing\n",
    "y=pd.get_dummies(train_df['airline_sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 8, validation_data = (X_test, y_test),batch_size=batch_size, verbose = 'auto')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"LSTM_details/model_structure_2.0.json\")\n",
    "f.write_text(model_structure)\n",
    "\n",
    "model.save_weights(\"LSTM_details/model_weights_2.0.h5\")\n",
    "\n",
    "model.save('LSTM_details/model_2.0.h5')\n",
    "\n",
    "max_length = str(len(X_train[0]))\n",
    "f = Path(\"LSTM_details/X_train_max_length_2.0.txt\")\n",
    "f.write_text(max_length)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df[['text_tokenized']].to_csv(\"LSTM_details/train_text_vals_2.0.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
